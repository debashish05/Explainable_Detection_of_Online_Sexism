{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Donwload torchtext this versiont**","metadata":{}},{"cell_type":"code","source":"!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/\n!pip install torchtext==0.10.1","metadata":{"execution":{"iopub.status.busy":"2023-04-17T21:20:17.513467Z","iopub.execute_input":"2023-04-17T21:20:17.514182Z","iopub.status.idle":"2023-04-17T21:21:29.706067Z","shell.execute_reply.started":"2023-04-17T21:20:17.514143Z","shell.execute_reply":"2023-04-17T21:21:29.704823Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Archive:  /usr/share/nltk_data/corpora/wordnet.zip\n   creating: /usr/share/nltk_data/corpora/wordnet/\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \nCollecting torchtext==0.10.1\n  Downloading torchtext-0.10.1-cp37-cp37m-manylinux1_x86_64.whl (7.6 MB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchtext==0.10.1) (2.28.2)\nCollecting torch==1.9.1\n  Downloading torch-1.9.1-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m831.4/831.4 MB\u001b[0m \u001b[31m770.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torchtext==0.10.1) (4.64.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchtext==0.10.1) (1.21.6)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.9.1->torchtext==0.10.1) (4.4.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.10.1) (1.26.14)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.10.1) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.10.1) (3.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.10.1) (2.1.1)\nInstalling collected packages: torch, torchtext\n  Attempting uninstall: torch\n    Found existing installation: torch 1.13.0\n    Uninstalling torch-1.13.0:\n      Successfully uninstalled torch-1.13.0\n  Attempting uninstall: torchtext\n    Found existing installation: torchtext 0.14.0\n    Uninstalling torchtext-0.14.0:\n      Successfully uninstalled torchtext-0.14.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npytorch-lightning 1.9.4 requires torch>=1.10.0, but you have torch 1.9.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed torch-1.9.1 torchtext-0.10.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport pandas as pd\nimport re\nimport string\nimport nltk\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport emoji\nimport torchtext\nimport random\nimport pandas as pd\nfrom torch.utils.data import DataLoader\nimport torch\nimport torch.nn.functional as F\nimport torchtext\nimport time\nimport random\nimport pandas as pd\nfrom torchtext.legacy import data\nfrom sklearn.metrics import classification_report\n\ntorch.manual_seed(1)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-17T21:21:29.709913Z","iopub.execute_input":"2023-04-17T21:21:29.710776Z","iopub.status.idle":"2023-04-17T21:21:31.424803Z","shell.execute_reply.started":"2023-04-17T21:21:29.710713Z","shell.execute_reply":"2023-04-17T21:21:31.423712Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x73169b30d310>"},"metadata":{}}]},{"cell_type":"markdown","source":"**GPU support available or not**","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T21:21:31.426388Z","iopub.execute_input":"2023-04-17T21:21:31.427076Z","iopub.status.idle":"2023-04-17T21:21:31.501610Z","shell.execute_reply.started":"2023-04-17T21:21:31.427037Z","shell.execute_reply":"2023-04-17T21:21:31.499394Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**HyperParameter**","metadata":{}},{"cell_type":"code","source":"LEARNING_RATE = 0.0003\nBATCH_SIZE = 128\nNUM_EPOCHS = 15\nEMBEDDING_DIM = 128\nHIDDEN_DIM = 256\nNUM_CLASSES = 11","metadata":{"execution":{"iopub.status.busy":"2023-04-17T21:55:43.808353Z","iopub.execute_input":"2023-04-17T21:55:43.809269Z","iopub.status.idle":"2023-04-17T21:55:43.814657Z","shell.execute_reply.started":"2023-04-17T21:55:43.809214Z","shell.execute_reply":"2023-04-17T21:55:43.813557Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"markdown","source":"**Preparing Data**","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/extradataset/edos_labelled_aggregated.csv')\ndf = df[df['label_sexist'] == \"sexist\"]\n\n# for task A we don't need these things\ndf.drop(['rewire_id','label_sexist','label_category'], axis=1, inplace=True)\n\ndfTrain=df[df['split']==\"train\"]\ndfVal=df[df['split']==\"dev\"]\ndfTest=df[df['split']==\"test\"]\n#df.drop(['rewire_id','label_category','label_vector'], axis=1, inplace=True)\n\ndfTrain.drop(['split'], axis=1, inplace=True)\ndfVal.drop(['split'], axis=1, inplace=True)\ndfTest.drop(['split'], axis=1, inplace=True)\n\nprint(dfTrain.head())\n\ndfTrain.to_csv('train.csv',index=False)\ndfTest.to_csv('test.csv',index=False)\ndfVal.to_csv('val.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T21:56:01.438416Z","iopub.execute_input":"2023-04-17T21:56:01.439348Z","iopub.status.idle":"2023-04-17T21:56:01.561419Z","shell.execute_reply.started":"2023-04-17T21:56:01.439301Z","shell.execute_reply":"2023-04-17T21:56:01.560417Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stdout","text":"                                                 text  \\\n6   [USER] Leg day is easy. Hot girls who wear min...   \n8   I get a new pussy every other week or whenever...   \n14  O come on there's no way any men are attracted...   \n26  Former BBC journalist Thuto Mali says that uni...   \n32  I did. You have to have the bravery to escalat...   \n\n                                         label_vector  \n6                 3.3 backhanded gendered compliments  \n8   2.3 dehumanising attacks & overt sexual object...  \n14  2.3 dehumanising attacks & overt sexual object...  \n26  4.2 supporting systemic discrimination against...  \n32           1.2 incitement and encouragement of harm  \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  errors=errors,\n","output_type":"stream"}]},{"cell_type":"code","source":"TEXT = data.Field(\n    tokenize='spacy', # default splits on whitespace\n    tokenizer_language='en_core_web_sm'\n)\n\n### Defining the label processing\nLABEL = data.LabelField(dtype=torch.long)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T21:56:05.358241Z","iopub.execute_input":"2023-04-17T21:56:05.359005Z","iopub.status.idle":"2023-04-17T21:56:05.921439Z","shell.execute_reply.started":"2023-04-17T21:56:05.358966Z","shell.execute_reply":"2023-04-17T21:56:05.920390Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"markdown","source":"**Loading Data**","metadata":{}},{"cell_type":"code","source":"fields = [('text', TEXT), ('label_vector', LABEL)]\ntrain_data = data.TabularDataset(path='/kaggle/working/train.csv', format='csv',skip_header=True, fields=fields)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T21:56:21.121664Z","iopub.execute_input":"2023-04-17T21:56:21.122368Z","iopub.status.idle":"2023-04-17T21:56:21.897145Z","shell.execute_reply.started":"2023-04-17T21:56:21.122331Z","shell.execute_reply":"2023-04-17T21:56:21.896102Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"test_data = data.TabularDataset(path='/kaggle/working/test.csv', format='csv',skip_header=True, fields=fields)\nvalid_data = data.TabularDataset(path='/kaggle/working/val.csv', format='csv',skip_header=True, fields=fields)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T21:56:24.597561Z","iopub.execute_input":"2023-04-17T21:56:24.597956Z","iopub.status.idle":"2023-04-17T21:56:24.844742Z","shell.execute_reply.started":"2023-04-17T21:56:24.597920Z","shell.execute_reply":"2023-04-17T21:56:24.843739Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"print(vars(train_data.examples[0]))","metadata":{"execution":{"iopub.status.busy":"2023-04-17T21:56:25.751196Z","iopub.execute_input":"2023-04-17T21:56:25.751559Z","iopub.status.idle":"2023-04-17T21:56:25.757909Z","shell.execute_reply.started":"2023-04-17T21:56:25.751527Z","shell.execute_reply":"2023-04-17T21:56:25.756676Z"},"trusted":true},"execution_count":81,"outputs":[{"name":"stdout","text":"{'text': ['[', 'USER', ']', 'Leg', 'day', 'is', 'easy', '.', 'Hot', 'girls', 'who', 'wear', 'miniskirts', 'get', 'asked', 'out', '.'], 'label_vector': '3.3 backhanded gendered compliments'}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Data Cleaning**: Not used","metadata":{}},{"cell_type":"code","source":"class DataCleaning():\n    \"\"\"\n        Take a list of strings and preprocess strings, it preforms:\n        \n    \"\"\"\n    def __init__(self, list_data):\n        self.data = list_data\n        self.len = len(list_data)\n\n    def clean_data(self):\n        self.emojis_to_text()\n        self.lowerCase()\n        self.linkToTag()\n        self.removePunctuations()\n        self.removeWordsWithNumber()\n        self.rootWord(lemmatizer=True)\n        self.removeStopword()\n        return self.data\n        \n    def tokenizer():\n        pass\n        \n    def rootWord(self,lemmatizer=False):\n        \"\"\"\n            Steamer is faster than lammatization.\n            Steamer cut the last few words, and use the root word\n            lemmatizer convert many form a word to the same word. \n            stemmer will retun 'car' for word 'caring'\n            while lemmatizer return 'care' for 'caring'\n        \"\"\"\n        if not lemmatizer:\n            stemmer = PorterStemmer()\n            for i in range(self.len):\n                self.data[i]=\" \".join([stemmer.stem(word) for word in self.data[i].split()])\n        else:\n            lemmatizer = WordNetLemmatizer()\n            for i in range(self.len):\n                self.data[i]=\" \".join([lemmatizer.lemmatize(word) for word in self.data[i].split()])\n        \n    def removePunctuations(self):\n        removePunc = re.compile(r'[^\\w\\s]')\n        for i in range(self.len):\n            self.data[i] = re.sub(removePunc, r\" \",self.data[i])\n\n    def emojis_to_text(self):\n        \"\"\"\n            Converting image to its text meaning.\n            Format: üëç to \":thumbs_up:\"\n        \"\"\"\n        for i in range(self.len):\n            self.data[i]=emoji.demojize(self.data[i])\n            \n    def linkToTag(self):\n        \"\"\"replacing web links with '<URL>'\"\"\"\n        linkRegex = re.compile(r'\\b(www|http|https)[^ |\\n]*')\n\n        for i in range(self.len):\n            self.data[i] = re.sub(linkRegex, r\"<URL>\",self.data[i])\n    \n    def removeWordsWithNumber(self):\n        \"\"\"\n            Zero or more number of non-whitespace then digit, \n            then Zero or more number of non-whitespace\n        \"\"\"\n        wordNumRegex = re.compile(r'\\S*\\d\\S*')\n        for i in range(self.len):\n            self.data[i]=re.sub(wordNumRegex,r\"\",self.data[i])\n    \n    def lowerCase(self):\n        \"\"\"\n            It is sometimes important to keep the letter capital \n            as it signifies shouting in form of that word, but it\n            depends on use case\n        \"\"\"\n        for i in range(self.len):\n            self.data[i]=self.data[i].lower()\n            \n    def removeStopword(self):\n        \"\"\" Remove stopward from a string\"\"\"\n        stop = stopwords.words('english')\n        for i in range(self.len):\n            self.data[i]=' '.join([word for word in self.data[i].split() if word not in stop])","metadata":{"execution":{"iopub.status.busy":"2023-04-17T21:56:27.983332Z","iopub.execute_input":"2023-04-17T21:56:27.983806Z","iopub.status.idle":"2023-04-17T21:56:28.003153Z","shell.execute_reply.started":"2023-04-17T21:56:27.983747Z","shell.execute_reply":"2023-04-17T21:56:28.001860Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"# a=DataCleaning(dfTrain['text'].tolist())\n# trainTexts=a.clean_data()\n# a=DataCleaning(dfTest['text'].tolist())\n# testTexts=a.clean_data()\n# a=DataCleaning(dfVal['text'].tolist())\n# valTexts=a.clean_data()","metadata":{"execution":{"iopub.status.busy":"2023-04-17T21:56:28.904660Z","iopub.execute_input":"2023-04-17T21:56:28.905831Z","iopub.status.idle":"2023-04-17T21:56:28.910676Z","shell.execute_reply.started":"2023-04-17T21:56:28.905771Z","shell.execute_reply":"2023-04-17T21:56:28.909431Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"# trainText=dfTrain['text'].tolist()\n# testText=dfTest['text'].tolist()\n# valText=dfVal['text'].tolist()","metadata":{"execution":{"iopub.status.busy":"2023-04-17T21:56:29.356996Z","iopub.execute_input":"2023-04-17T21:56:29.357982Z","iopub.status.idle":"2023-04-17T21:56:29.363187Z","shell.execute_reply.started":"2023-04-17T21:56:29.357935Z","shell.execute_reply":"2023-04-17T21:56:29.361821Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"len(train_data)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T21:56:30.256571Z","iopub.execute_input":"2023-04-17T21:56:30.257688Z","iopub.status.idle":"2023-04-17T21:56:30.265853Z","shell.execute_reply.started":"2023-04-17T21:56:30.257632Z","shell.execute_reply":"2023-04-17T21:56:30.264804Z"},"trusted":true},"execution_count":85,"outputs":[{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"3398"},"metadata":{}}]},{"cell_type":"markdown","source":"**Build vocab**","metadata":{}},{"cell_type":"code","source":"TEXT.build_vocab(train_data)\nLABEL.build_vocab(train_data)\n\nprint(f'Vocabulary size: {len(TEXT.vocab)}')\nprint(f'Number of classes: {len(LABEL.vocab)}')","metadata":{"execution":{"iopub.status.busy":"2023-04-17T21:56:34.224641Z","iopub.execute_input":"2023-04-17T21:56:34.225333Z","iopub.status.idle":"2023-04-17T21:56:34.273300Z","shell.execute_reply.started":"2023-04-17T21:56:34.225296Z","shell.execute_reply":"2023-04-17T21:56:34.272202Z"},"trusted":true},"execution_count":86,"outputs":[{"name":"stdout","text":"Vocabulary size: 10905\nNumber of classes: 11\n","output_type":"stream"}]},{"cell_type":"code","source":"print(LABEL.vocab.stoi)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T21:56:40.261455Z","iopub.execute_input":"2023-04-17T21:56:40.261860Z","iopub.status.idle":"2023-04-17T21:56:40.267488Z","shell.execute_reply.started":"2023-04-17T21:56:40.261825Z","shell.execute_reply":"2023-04-17T21:56:40.266437Z"},"trusted":true},"execution_count":87,"outputs":[{"name":"stdout","text":"defaultdict(None, {'2.1 descriptive attacks': 0, '2.2 aggressive and emotive attacks': 1, '3.1 casual use of gendered slurs, profanities, and insults': 2, '3.2 immutable gender differences and gender stereotypes': 3, '4.2 supporting systemic discrimination against women as a group': 4, '1.2 incitement and encouragement of harm': 5, '2.3 dehumanising attacks & overt sexual objectification': 6, '4.1 supporting mistreatment of individual women': 7, '3.3 backhanded gendered compliments': 8, '1.1 threats of harm': 9, '3.4 condescending explanations or unwelcome advice': 10})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Dataloader**","metadata":{}},{"cell_type":"code","source":"train_loader, valid_loader, test_loader = \\\n    data.BucketIterator.splits(\n        (train_data, valid_data, test_data),\n         batch_size=BATCH_SIZE,\n         sort_within_batch=False,\n         sort_key=lambda x: len(x.text),\n         device=device\n    )","metadata":{"execution":{"iopub.status.busy":"2023-04-17T21:56:43.846720Z","iopub.execute_input":"2023-04-17T21:56:43.847115Z","iopub.status.idle":"2023-04-17T21:56:43.853315Z","shell.execute_reply.started":"2023-04-17T21:56:43.847076Z","shell.execute_reply":"2023-04-17T21:56:43.852274Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"markdown","source":"**Shape of data**","metadata":{}},{"cell_type":"code","source":"print('Train')\nfor batch in train_loader:\n    print(f'Text matrix size: {batch.text.size()}')\n    print(f'Target vector size: {batch.label_vector.size()}')\n    break\n    \nprint('\\nValid:')\nfor batch in valid_loader:\n    print(f'Text matrix size: {batch.text.size()}')\n    print(f'Target vector size: {batch.label_vector.size()}')\n    break\n    \nprint('\\nTest:')\nfor batch in test_loader:\n    print(f'Text matrix size: {batch.text.size()}')\n    print(f'Target vector size: {batch.label_vector.size()}')\n    break","metadata":{"execution":{"iopub.status.busy":"2023-04-17T21:56:56.295615Z","iopub.execute_input":"2023-04-17T21:56:56.296295Z","iopub.status.idle":"2023-04-17T21:56:56.321309Z","shell.execute_reply.started":"2023-04-17T21:56:56.296260Z","shell.execute_reply":"2023-04-17T21:56:56.320154Z"},"trusted":true},"execution_count":89,"outputs":[{"name":"stdout","text":"Train\nText matrix size: torch.Size([56, 128])\nTarget vector size: torch.Size([128])\n\nValid:\nText matrix size: torch.Size([19, 128])\nTarget vector size: torch.Size([128])\n\nTest:\nText matrix size: torch.Size([13, 128])\nTarget vector size: torch.Size([128])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Module**","metadata":{}},{"cell_type":"code","source":"class RNN(torch.nn.Module):\n    \n    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n        super().__init__()\n\n        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)\n        #self.rnn = torch.nn.RNN(embedding_dim,\n        #                        hidden_dim,\n        #                        nonlinearity='relu')\n        self.rnn = torch.nn.LSTM(embedding_dim,\n                                 hidden_dim)        \n        \n        self.fc1 = torch.nn.Linear(hidden_dim, hidden_dim//2)\n        self.fc2 = torch.nn.Linear(hidden_dim//2, output_dim)\n        \n\n    def forward(self, text):\n        # text dim: [sentence length, batch size]\n        \n        embedded = self.embedding(text)\n        # embedded dim: [sentence length, batch size, embedding dim]\n        \n        output, (hidden, cell) = self.rnn(embedded)\n        # output dim: [sentence length, batch size, hidden dim]\n        # hidden dim: [1, batch size, hidden dim]\n\n        hidden.squeeze_(0)\n        # hidden dim: [batch size, hidden dim]\n        \n        output = self.fc1(hidden)\n        output = self.fc2(F.relu(output))\n        return output","metadata":{"execution":{"iopub.status.busy":"2023-04-17T21:57:02.490619Z","iopub.execute_input":"2023-04-17T21:57:02.491324Z","iopub.status.idle":"2023-04-17T21:57:02.499410Z","shell.execute_reply.started":"2023-04-17T21:57:02.491284Z","shell.execute_reply":"2023-04-17T21:57:02.497988Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"model = RNN(input_dim=len(TEXT.vocab),\n            embedding_dim=EMBEDDING_DIM,\n            hidden_dim=HIDDEN_DIM,\n            output_dim=NUM_CLASSES # could use 1 for binary classification\n)\n\nmodel = model.to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T21:57:08.655054Z","iopub.execute_input":"2023-04-17T21:57:08.656002Z","iopub.status.idle":"2023-04-17T21:57:08.678713Z","shell.execute_reply.started":"2023-04-17T21:57:08.655950Z","shell.execute_reply":"2023-04-17T21:57:08.677688Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"markdown","source":"**Accuracy Report**","metadata":{}},{"cell_type":"code","source":"def compute_accuracy(model, data_loader, device,report=False):\n\n    predict=[]\n    actual=[]\n    \n    with torch.no_grad():\n\n        correct_pred, num_examples = 0, 0\n\n        for i, (features, targets) in enumerate(data_loader):\n\n            features = features.to(device)\n            targets = targets.float().to(device)\n\n            logits = model(features)\n            _, predicted_labels = torch.max(logits, 1)\n\n            num_examples += targets.size(0)\n            correct_pred += (predicted_labels == targets).sum()\n            \n            predict+=predicted_labels.tolist()                         \n            actual+=targets.tolist()\n            \n        if report:    \n            print(classification_report(actual, predict, labels=[0,1]))\n            \n    return correct_pred.float()/num_examples * 100","metadata":{"execution":{"iopub.status.busy":"2023-04-17T21:57:17.816465Z","iopub.execute_input":"2023-04-17T21:57:17.817191Z","iopub.status.idle":"2023-04-17T21:57:17.825099Z","shell.execute_reply.started":"2023-04-17T21:57:17.817151Z","shell.execute_reply":"2023-04-17T21:57:17.823637Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"DEVICE=device","metadata":{"execution":{"iopub.status.busy":"2023-04-17T21:57:19.009423Z","iopub.execute_input":"2023-04-17T21:57:19.009835Z","iopub.status.idle":"2023-04-17T21:57:19.014775Z","shell.execute_reply.started":"2023-04-17T21:57:19.009778Z","shell.execute_reply":"2023-04-17T21:57:19.013439Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"markdown","source":"**Training**","metadata":{}},{"cell_type":"code","source":"start_time = time.time()\n\nfor epoch in range(NUM_EPOCHS):\n    model.train()\n    for batch_idx, batch_data in enumerate(train_loader):\n        \n        text = batch_data.text.to(device)\n        labels = batch_data.label_vector.to(device)\n\n        ### FORWARD AND BACK PROP\n        logits = model(text)\n        loss = F.cross_entropy(logits, labels)\n        optimizer.zero_grad()\n        \n        loss.backward()\n        \n        ### UPDATE MODEL PARAMETERS\n        optimizer.step()\n        \n        ### LOGGING\n        if not batch_idx % 50:\n            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n                   f'Batch {batch_idx:03d}/{len(train_loader):03d} | '\n                   f'Loss: {loss:.4f}')\n\n    torch.save(model.state_dict(), 'lstm'+str(epoch)+\".pt\")\n            \n    with torch.set_grad_enabled(False):\n        print(f'training accuracy: '\n              f'{compute_accuracy(model, train_loader, device):.2f}%'\n              f'\\nvalid accuracy: '\n              f'{compute_accuracy(model, valid_loader, device):.2f}%')\n        \n    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n    \nprint(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\nprint(f'Test accuracy: {compute_accuracy(model, test_loader, device):.2f}%')","metadata":{"execution":{"iopub.status.busy":"2023-04-17T21:57:28.493551Z","iopub.execute_input":"2023-04-17T21:57:28.493931Z","iopub.status.idle":"2023-04-17T21:57:35.507393Z","shell.execute_reply.started":"2023-04-17T21:57:28.493898Z","shell.execute_reply":"2023-04-17T21:57:35.506311Z"},"trusted":true},"execution_count":95,"outputs":[{"name":"stdout","text":"Epoch: 001/015 | Batch 000/027 | Loss: 2.3987\ntraining accuracy: 21.13%\nvalid accuracy: 21.19%\nTime elapsed: 0.01 min\nEpoch: 002/015 | Batch 000/027 | Loss: 2.1050\ntraining accuracy: 18.72%\nvalid accuracy: 18.93%\nTime elapsed: 0.02 min\nEpoch: 003/015 | Batch 000/027 | Loss: 2.0951\ntraining accuracy: 21.25%\nvalid accuracy: 21.19%\nTime elapsed: 0.02 min\nEpoch: 004/015 | Batch 000/027 | Loss: 2.0531\ntraining accuracy: 21.31%\nvalid accuracy: 20.78%\nTime elapsed: 0.03 min\nEpoch: 005/015 | Batch 000/027 | Loss: 2.0617\ntraining accuracy: 21.31%\nvalid accuracy: 20.78%\nTime elapsed: 0.04 min\nEpoch: 006/015 | Batch 000/027 | Loss: 2.0043\ntraining accuracy: 20.04%\nvalid accuracy: 19.55%\nTime elapsed: 0.05 min\nEpoch: 007/015 | Batch 000/027 | Loss: 1.9484\ntraining accuracy: 21.22%\nvalid accuracy: 20.99%\nTime elapsed: 0.05 min\nEpoch: 008/015 | Batch 000/027 | Loss: 2.1134\ntraining accuracy: 21.28%\nvalid accuracy: 20.78%\nTime elapsed: 0.06 min\nEpoch: 009/015 | Batch 000/027 | Loss: 2.0921\ntraining accuracy: 18.81%\nvalid accuracy: 18.72%\nTime elapsed: 0.07 min\nEpoch: 010/015 | Batch 000/027 | Loss: 1.9856\ntraining accuracy: 21.42%\nvalid accuracy: 21.19%\nTime elapsed: 0.08 min\nEpoch: 011/015 | Batch 000/027 | Loss: 2.1042\ntraining accuracy: 18.81%\nvalid accuracy: 18.72%\nTime elapsed: 0.09 min\nEpoch: 012/015 | Batch 000/027 | Loss: 2.0705\ntraining accuracy: 21.42%\nvalid accuracy: 21.19%\nTime elapsed: 0.09 min\nEpoch: 013/015 | Batch 000/027 | Loss: 2.0458\ntraining accuracy: 21.42%\nvalid accuracy: 20.99%\nTime elapsed: 0.10 min\nEpoch: 014/015 | Batch 000/027 | Loss: 1.9597\ntraining accuracy: 21.34%\nvalid accuracy: 21.60%\nTime elapsed: 0.11 min\nEpoch: 015/015 | Batch 000/027 | Loss: 2.0951\ntraining accuracy: 21.37%\nvalid accuracy: 21.81%\nTime elapsed: 0.12 min\nTotal Training Time: 0.12 min\nTest accuracy: 21.44%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Loading the best Model**","metadata":{}},{"cell_type":"code","source":"path = \"/kaggle/working/lstm14.pt\"\nmodel.load_state_dict(torch.load(path))","metadata":{"execution":{"iopub.status.busy":"2023-04-17T21:57:56.396746Z","iopub.execute_input":"2023-04-17T21:57:56.397741Z","iopub.status.idle":"2023-04-17T21:57:56.412359Z","shell.execute_reply.started":"2023-04-17T21:57:56.397705Z","shell.execute_reply":"2023-04-17T21:57:56.411223Z"},"trusted":true},"execution_count":96,"outputs":[{"execution_count":96,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"markdown","source":"**F1 score at the test data**","metadata":{}},{"cell_type":"code","source":"with torch.set_grad_enabled(False):\n    print('test accuracy: ',compute_accuracy(model, test_loader, device,True))","metadata":{"execution":{"iopub.status.busy":"2023-04-17T21:57:59.538204Z","iopub.execute_input":"2023-04-17T21:57:59.538840Z","iopub.status.idle":"2023-04-17T21:57:59.584673Z","shell.execute_reply.started":"2023-04-17T21:57:59.538797Z","shell.execute_reply":"2023-04-17T21:57:59.583578Z"},"trusted":true},"execution_count":97,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.21      0.79      0.34       205\n           1       0.00      0.00      0.00       192\n\n   micro avg       0.21      0.41      0.28       397\n   macro avg       0.11      0.39      0.17       397\nweighted avg       0.11      0.41      0.17       397\n\ntest accuracy:  tensor(21.4433, device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"with torch.set_grad_enabled(False):\n    print('test accuracy: ',compute_accuracy(model, valid_loader, device,True))","metadata":{"execution":{"iopub.status.busy":"2023-04-17T21:58:01.342359Z","iopub.execute_input":"2023-04-17T21:58:01.343044Z","iopub.status.idle":"2023-04-17T21:58:01.373957Z","shell.execute_reply.started":"2023-04-17T21:58:01.343007Z","shell.execute_reply":"2023-04-17T21:58:01.372913Z"},"trusted":true},"execution_count":98,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.21      0.92      0.35       102\n           1       0.00      0.00      0.00        96\n\n   micro avg       0.21      0.47      0.30       198\n   macro avg       0.11      0.46      0.17       198\nweighted avg       0.11      0.47      0.18       198\n\ntest accuracy:  tensor(21.8107, device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Macro F1 score is 0.17","metadata":{}},{"cell_type":"markdown","source":"Reference\n\n1. https://www.youtube.com/watch?v=CrS-LFXEiyk\n2. https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n3. https://github.com/rasbt/stat453-deep-learning-ss21/blob/main/L15/1_lstm.ipynb","metadata":{}}]}