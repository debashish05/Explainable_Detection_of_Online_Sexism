{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Donwload torchtext this versiont**","metadata":{}},{"cell_type":"code","source":"!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/\n!pip install torchtext==0.10.1","metadata":{"execution":{"iopub.status.busy":"2023-04-17T21:20:17.513467Z","iopub.execute_input":"2023-04-17T21:20:17.514182Z","iopub.status.idle":"2023-04-17T21:21:29.706067Z","shell.execute_reply.started":"2023-04-17T21:20:17.514143Z","shell.execute_reply":"2023-04-17T21:21:29.704823Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Archive:  /usr/share/nltk_data/corpora/wordnet.zip\n   creating: /usr/share/nltk_data/corpora/wordnet/\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \nCollecting torchtext==0.10.1\n  Downloading torchtext-0.10.1-cp37-cp37m-manylinux1_x86_64.whl (7.6 MB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchtext==0.10.1) (2.28.2)\nCollecting torch==1.9.1\n  Downloading torch-1.9.1-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m831.4/831.4 MB\u001b[0m \u001b[31m770.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torchtext==0.10.1) (4.64.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchtext==0.10.1) (1.21.6)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.9.1->torchtext==0.10.1) (4.4.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.10.1) (1.26.14)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.10.1) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.10.1) (3.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.10.1) (2.1.1)\nInstalling collected packages: torch, torchtext\n  Attempting uninstall: torch\n    Found existing installation: torch 1.13.0\n    Uninstalling torch-1.13.0:\n      Successfully uninstalled torch-1.13.0\n  Attempting uninstall: torchtext\n    Found existing installation: torchtext 0.14.0\n    Uninstalling torchtext-0.14.0:\n      Successfully uninstalled torchtext-0.14.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npytorch-lightning 1.9.4 requires torch>=1.10.0, but you have torch 1.9.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed torch-1.9.1 torchtext-0.10.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport pandas as pd\nimport re\nimport string\nimport nltk\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport emoji\nimport torchtext\nimport random\nimport pandas as pd\nfrom torch.utils.data import DataLoader\nimport torch\nimport torch.nn.functional as F\nimport torchtext\nimport time\nimport random\nimport pandas as pd\nfrom torchtext.legacy import data\nfrom sklearn.metrics import classification_report\n\ntorch.manual_seed(1)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-17T22:00:35.714305Z","iopub.execute_input":"2023-04-17T22:00:35.714921Z","iopub.status.idle":"2023-04-17T22:00:35.725092Z","shell.execute_reply.started":"2023-04-17T22:00:35.714882Z","shell.execute_reply":"2023-04-17T22:00:35.723918Z"},"trusted":true},"execution_count":99,"outputs":[{"execution_count":99,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x73169b30d310>"},"metadata":{}}]},{"cell_type":"markdown","source":"**GPU support available or not**","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T22:00:38.048245Z","iopub.execute_input":"2023-04-17T22:00:38.048995Z","iopub.status.idle":"2023-04-17T22:00:38.055347Z","shell.execute_reply.started":"2023-04-17T22:00:38.048957Z","shell.execute_reply":"2023-04-17T22:00:38.054223Z"},"trusted":true},"execution_count":100,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**HyperParameter**","metadata":{}},{"cell_type":"code","source":"LEARNING_RATE = 0.0003\nBATCH_SIZE = 128\nNUM_EPOCHS = 15\nEMBEDDING_DIM = 128\nHIDDEN_DIM = 256\nNUM_CLASSES = 2","metadata":{"execution":{"iopub.status.busy":"2023-04-17T22:00:39.449267Z","iopub.execute_input":"2023-04-17T22:00:39.449632Z","iopub.status.idle":"2023-04-17T22:00:39.454654Z","shell.execute_reply.started":"2023-04-17T22:00:39.449601Z","shell.execute_reply":"2023-04-17T22:00:39.453479Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"markdown","source":"**Preparing Data**","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/extradataset/edos_labelled_aggregated.csv')\n\n# for task A we don't need these things\ndf.drop(['rewire_id','label_category','label_vector'], axis=1, inplace=True)\n\ndfTrain=df[df['split']==\"train\"]\ndfVal=df[df['split']==\"dev\"]\ndfTest=df[df['split']==\"test\"]\n#df.drop(['rewire_id','label_category','label_vector'], axis=1, inplace=True)\n\ndfTrain.drop(['split'], axis=1, inplace=True)\ndfVal.drop(['split'], axis=1, inplace=True)\ndfTest.drop(['split'], axis=1, inplace=True)\n\nprint(dfTrain.head())\n\ndfTrain.to_csv('train.csv',index=False)\ndfTest.to_csv('test.csv',index=False)\ndfVal.to_csv('val.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T22:00:41.043376Z","iopub.execute_input":"2023-04-17T22:00:41.043977Z","iopub.status.idle":"2023-04-17T22:00:41.186703Z","shell.execute_reply.started":"2023-04-17T22:00:41.043939Z","shell.execute_reply":"2023-04-17T22:00:41.185720Z"},"trusted":true},"execution_count":102,"outputs":[{"name":"stdout","text":"                                                text label_sexist\n1                            Then, she's a keeper. üòâ   not sexist\n2  This is like the Metallica video where the poo...   not sexist\n3                                             woman?   not sexist\n5  Unlicensed day care worker reportedly tells co...   not sexist\n6  [USER] Leg day is easy. Hot girls who wear min...       sexist\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py:4913: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  errors=errors,\n","output_type":"stream"}]},{"cell_type":"code","source":"TEXT = data.Field(\n    tokenize='spacy', # default splits on whitespace\n    tokenizer_language='en_core_web_sm'\n)\n\n### Defining the label processing\nLABEL = data.LabelField(dtype=torch.long)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T22:00:41.760505Z","iopub.execute_input":"2023-04-17T22:00:41.761628Z","iopub.status.idle":"2023-04-17T22:00:42.306478Z","shell.execute_reply.started":"2023-04-17T22:00:41.761581Z","shell.execute_reply":"2023-04-17T22:00:42.305421Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"markdown","source":"**Loading Data**","metadata":{}},{"cell_type":"code","source":"fields = [('text', TEXT), ('label_sexist', LABEL)]\ntrain_data = data.TabularDataset(path='/kaggle/working/train.csv', format='csv',skip_header=True, fields=fields)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T22:00:43.193612Z","iopub.execute_input":"2023-04-17T22:00:43.194314Z","iopub.status.idle":"2023-04-17T22:00:45.527502Z","shell.execute_reply.started":"2023-04-17T22:00:43.194276Z","shell.execute_reply":"2023-04-17T22:00:45.526495Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"test_data = data.TabularDataset(path='/kaggle/working/test.csv', format='csv',skip_header=True, fields=fields)\nvalid_data = data.TabularDataset(path='/kaggle/working/val.csv', format='csv',skip_header=True, fields=fields)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T22:00:45.529593Z","iopub.execute_input":"2023-04-17T22:00:45.529983Z","iopub.status.idle":"2023-04-17T22:00:46.307737Z","shell.execute_reply.started":"2023-04-17T22:00:45.529944Z","shell.execute_reply":"2023-04-17T22:00:46.306730Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"print(vars(train_data.examples[0]))","metadata":{"execution":{"iopub.status.busy":"2023-04-17T22:00:46.308986Z","iopub.execute_input":"2023-04-17T22:00:46.310013Z","iopub.status.idle":"2023-04-17T22:00:46.315965Z","shell.execute_reply.started":"2023-04-17T22:00:46.309983Z","shell.execute_reply":"2023-04-17T22:00:46.314822Z"},"trusted":true},"execution_count":106,"outputs":[{"name":"stdout","text":"{'text': ['Then', ',', 'she', \"'s\", 'a', 'keeper', '.', 'üòâ'], 'label_sexist': 'not sexist'}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Data Cleaning**: Not used","metadata":{}},{"cell_type":"code","source":"class DataCleaning():\n    \"\"\"\n        Take a list of strings and preprocess strings, it preforms:\n        \n    \"\"\"\n    def __init__(self, list_data):\n        self.data = list_data\n        self.len = len(list_data)\n\n    def clean_data(self):\n        self.emojis_to_text()\n        self.lowerCase()\n        self.linkToTag()\n        self.removePunctuations()\n        self.removeWordsWithNumber()\n        self.rootWord(lemmatizer=True)\n        self.removeStopword()\n        return self.data\n        \n    def tokenizer():\n        pass\n        \n    def rootWord(self,lemmatizer=False):\n        \"\"\"\n            Steamer is faster than lammatization.\n            Steamer cut the last few words, and use the root word\n            lemmatizer convert many form a word to the same word. \n            stemmer will retun 'car' for word 'caring'\n            while lemmatizer return 'care' for 'caring'\n        \"\"\"\n        if not lemmatizer:\n            stemmer = PorterStemmer()\n            for i in range(self.len):\n                self.data[i]=\" \".join([stemmer.stem(word) for word in self.data[i].split()])\n        else:\n            lemmatizer = WordNetLemmatizer()\n            for i in range(self.len):\n                self.data[i]=\" \".join([lemmatizer.lemmatize(word) for word in self.data[i].split()])\n        \n    def removePunctuations(self):\n        removePunc = re.compile(r'[^\\w\\s]')\n        for i in range(self.len):\n            self.data[i] = re.sub(removePunc, r\" \",self.data[i])\n\n    def emojis_to_text(self):\n        \"\"\"\n            Converting image to its text meaning.\n            Format: üëç to \":thumbs_up:\"\n        \"\"\"\n        for i in range(self.len):\n            self.data[i]=emoji.demojize(self.data[i])\n            \n    def linkToTag(self):\n        \"\"\"replacing web links with '<URL>'\"\"\"\n        linkRegex = re.compile(r'\\b(www|http|https)[^ |\\n]*')\n\n        for i in range(self.len):\n            self.data[i] = re.sub(linkRegex, r\"<URL>\",self.data[i])\n    \n    def removeWordsWithNumber(self):\n        \"\"\"\n            Zero or more number of non-whitespace then digit, \n            then Zero or more number of non-whitespace\n        \"\"\"\n        wordNumRegex = re.compile(r'\\S*\\d\\S*')\n        for i in range(self.len):\n            self.data[i]=re.sub(wordNumRegex,r\"\",self.data[i])\n    \n    def lowerCase(self):\n        \"\"\"\n            It is sometimes important to keep the letter capital \n            as it signifies shouting in form of that word, but it\n            depends on use case\n        \"\"\"\n        for i in range(self.len):\n            self.data[i]=self.data[i].lower()\n            \n    def removeStopword(self):\n        \"\"\" Remove stopward from a string\"\"\"\n        stop = stopwords.words('english')\n        for i in range(self.len):\n            self.data[i]=' '.join([word for word in self.data[i].split() if word not in stop])","metadata":{"execution":{"iopub.status.busy":"2023-04-17T22:00:46.319039Z","iopub.execute_input":"2023-04-17T22:00:46.319899Z","iopub.status.idle":"2023-04-17T22:00:46.336238Z","shell.execute_reply.started":"2023-04-17T22:00:46.319858Z","shell.execute_reply":"2023-04-17T22:00:46.335219Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"# a=DataCleaning(dfTrain['text'].tolist())\n# trainTexts=a.clean_data()\n# a=DataCleaning(dfTest['text'].tolist())\n# testTexts=a.clean_data()\n# a=DataCleaning(dfVal['text'].tolist())\n# valTexts=a.clean_data()","metadata":{"execution":{"iopub.status.busy":"2023-04-17T22:00:46.339602Z","iopub.execute_input":"2023-04-17T22:00:46.339939Z","iopub.status.idle":"2023-04-17T22:00:46.349711Z","shell.execute_reply.started":"2023-04-17T22:00:46.339895Z","shell.execute_reply":"2023-04-17T22:00:46.348703Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"# trainText=dfTrain['text'].tolist()\n# testText=dfTest['text'].tolist()\n# valText=dfVal['text'].tolist()","metadata":{"execution":{"iopub.status.busy":"2023-04-17T22:00:46.351059Z","iopub.execute_input":"2023-04-17T22:00:46.352320Z","iopub.status.idle":"2023-04-17T22:00:46.359147Z","shell.execute_reply.started":"2023-04-17T22:00:46.352283Z","shell.execute_reply":"2023-04-17T22:00:46.358177Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"len(train_data)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T22:00:46.607462Z","iopub.execute_input":"2023-04-17T22:00:46.608605Z","iopub.status.idle":"2023-04-17T22:00:46.615566Z","shell.execute_reply.started":"2023-04-17T22:00:46.608558Z","shell.execute_reply":"2023-04-17T22:00:46.614371Z"},"trusted":true},"execution_count":110,"outputs":[{"execution_count":110,"output_type":"execute_result","data":{"text/plain":"14000"},"metadata":{}}]},{"cell_type":"markdown","source":"**Build vocab**","metadata":{}},{"cell_type":"code","source":"TEXT.build_vocab(train_data)\nLABEL.build_vocab(train_data)\n\nprint(f'Vocabulary size: {len(TEXT.vocab)}')\nprint(f'Number of classes: {len(LABEL.vocab)}')","metadata":{"execution":{"iopub.status.busy":"2023-04-17T22:00:47.477003Z","iopub.execute_input":"2023-04-17T22:00:47.477700Z","iopub.status.idle":"2023-04-17T22:00:47.634720Z","shell.execute_reply.started":"2023-04-17T22:00:47.477664Z","shell.execute_reply":"2023-04-17T22:00:47.633473Z"},"trusted":true},"execution_count":111,"outputs":[{"name":"stdout","text":"Vocabulary size: 26887\nNumber of classes: 2\n","output_type":"stream"}]},{"cell_type":"code","source":"print(LABEL.vocab.stoi)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T22:00:48.165867Z","iopub.execute_input":"2023-04-17T22:00:48.166555Z","iopub.status.idle":"2023-04-17T22:00:48.171942Z","shell.execute_reply.started":"2023-04-17T22:00:48.166518Z","shell.execute_reply":"2023-04-17T22:00:48.170748Z"},"trusted":true},"execution_count":112,"outputs":[{"name":"stdout","text":"defaultdict(None, {'not sexist': 0, 'sexist': 1})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Dataloader**","metadata":{}},{"cell_type":"code","source":"train_loader, valid_loader, test_loader = \\\n    data.BucketIterator.splits(\n        (train_data, valid_data, test_data),\n         batch_size=BATCH_SIZE,\n         sort_within_batch=False,\n         sort_key=lambda x: len(x.text),\n         device=device\n    )","metadata":{"execution":{"iopub.status.busy":"2023-04-17T22:00:56.216927Z","iopub.execute_input":"2023-04-17T22:00:56.217306Z","iopub.status.idle":"2023-04-17T22:00:56.223687Z","shell.execute_reply.started":"2023-04-17T22:00:56.217273Z","shell.execute_reply":"2023-04-17T22:00:56.222369Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"markdown","source":"**Shape of data**","metadata":{}},{"cell_type":"code","source":"print('Train')\nfor batch in train_loader:\n    print(f'Text matrix size: {batch.text.size()}')\n    print(f'Target vector size: {batch.label_sexist.size()}')\n    break\n    \nprint('\\nValid:')\nfor batch in valid_loader:\n    print(f'Text matrix size: {batch.text.size()}')\n    print(f'Target vector size: {batch.label_sexist.size()}')\n    break\n    \nprint('\\nTest:')\nfor batch in test_loader:\n    print(f'Text matrix size: {batch.text.size()}')\n    print(f'Target vector size: {batch.label_sexist.size()}')\n    break","metadata":{"execution":{"iopub.status.busy":"2023-04-17T22:00:57.888798Z","iopub.execute_input":"2023-04-17T22:00:57.889718Z","iopub.status.idle":"2023-04-17T22:00:57.937301Z","shell.execute_reply.started":"2023-04-17T22:00:57.889666Z","shell.execute_reply":"2023-04-17T22:00:57.935872Z"},"trusted":true},"execution_count":114,"outputs":[{"name":"stdout","text":"Train\nText matrix size: torch.Size([53, 128])\nTarget vector size: torch.Size([128])\n\nValid:\nText matrix size: torch.Size([9, 128])\nTarget vector size: torch.Size([128])\n\nTest:\nText matrix size: torch.Size([6, 128])\nTarget vector size: torch.Size([128])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Module**","metadata":{}},{"cell_type":"code","source":"class RNN(torch.nn.Module):\n    \n    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n        super().__init__()\n\n        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)\n        #self.rnn = torch.nn.RNN(embedding_dim,\n        #                        hidden_dim,\n        #                        nonlinearity='relu')\n        self.rnn = torch.nn.LSTM(embedding_dim,\n                                 hidden_dim)        \n        \n        self.fc1 = torch.nn.Linear(hidden_dim, hidden_dim//2)\n        self.fc2 = torch.nn.Linear(hidden_dim//2, output_dim)\n        \n\n    def forward(self, text):\n        # text dim: [sentence length, batch size]\n        \n        embedded = self.embedding(text)\n        # embedded dim: [sentence length, batch size, embedding dim]\n        \n        output, (hidden, cell) = self.rnn(embedded)\n        # output dim: [sentence length, batch size, hidden dim]\n        # hidden dim: [1, batch size, hidden dim]\n\n        hidden.squeeze_(0)\n        # hidden dim: [batch size, hidden dim]\n        \n        output = self.fc1(hidden)\n        output = self.fc2(F.relu(output))\n        return output","metadata":{"execution":{"iopub.status.busy":"2023-04-17T22:01:00.110905Z","iopub.execute_input":"2023-04-17T22:01:00.111324Z","iopub.status.idle":"2023-04-17T22:01:00.120014Z","shell.execute_reply.started":"2023-04-17T22:01:00.111292Z","shell.execute_reply":"2023-04-17T22:01:00.118806Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"model = RNN(input_dim=len(TEXT.vocab),\n            embedding_dim=EMBEDDING_DIM,\n            hidden_dim=HIDDEN_DIM,\n            output_dim=2 # could use 1 for binary classification\n)\n\nmodel = model.to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T22:01:00.818551Z","iopub.execute_input":"2023-04-17T22:01:00.818938Z","iopub.status.idle":"2023-04-17T22:01:00.864636Z","shell.execute_reply.started":"2023-04-17T22:01:00.818904Z","shell.execute_reply":"2023-04-17T22:01:00.862745Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"markdown","source":"**Accuracy Report**","metadata":{}},{"cell_type":"code","source":"def compute_accuracy(model, data_loader, device,report=False):\n\n    predict=[]\n    actual=[]\n    \n    with torch.no_grad():\n\n        correct_pred, num_examples = 0, 0\n\n        for i, (features, targets) in enumerate(data_loader):\n\n            features = features.to(device)\n            targets = targets.float().to(device)\n\n            logits = model(features)\n            _, predicted_labels = torch.max(logits, 1)\n\n            num_examples += targets.size(0)\n            correct_pred += (predicted_labels == targets).sum()\n            \n            predict+=predicted_labels.tolist()                         \n            actual+=targets.tolist()\n            \n        if report:    \n            print(classification_report(actual, predict, labels=[0,1]))\n            \n    return correct_pred.float()/num_examples * 100","metadata":{"execution":{"iopub.status.busy":"2023-04-17T22:01:02.829829Z","iopub.execute_input":"2023-04-17T22:01:02.830515Z","iopub.status.idle":"2023-04-17T22:01:02.838111Z","shell.execute_reply.started":"2023-04-17T22:01:02.830480Z","shell.execute_reply":"2023-04-17T22:01:02.836830Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"DEVICE=device","metadata":{"execution":{"iopub.status.busy":"2023-04-17T22:01:03.625256Z","iopub.execute_input":"2023-04-17T22:01:03.626016Z","iopub.status.idle":"2023-04-17T22:01:03.632972Z","shell.execute_reply.started":"2023-04-17T22:01:03.625977Z","shell.execute_reply":"2023-04-17T22:01:03.631822Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"markdown","source":"**Training**","metadata":{}},{"cell_type":"code","source":"start_time = time.time()\n\nfor epoch in range(NUM_EPOCHS):\n    model.train()\n    for batch_idx, batch_data in enumerate(train_loader):\n        \n        text = batch_data.text.to(device)\n        labels = batch_data.label_sexist.to(device)\n\n        ### FORWARD AND BACK PROP\n        logits = model(text)\n        loss = F.cross_entropy(logits, labels)\n        optimizer.zero_grad()\n        \n        loss.backward()\n        \n        ### UPDATE MODEL PARAMETERS\n        optimizer.step()\n        \n        ### LOGGING\n        if not batch_idx % 50:\n            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n                   f'Batch {batch_idx:03d}/{len(train_loader):03d} | '\n                   f'Loss: {loss:.4f}')\n\n    torch.save(model.state_dict(), 'lstm'+str(epoch)+\".pt\")\n            \n    with torch.set_grad_enabled(False):\n        print(f'training accuracy: '\n              f'{compute_accuracy(model, train_loader, device):.2f}%'\n              f'\\nvalid accuracy: '\n              f'{compute_accuracy(model, valid_loader, device):.2f}%')\n        \n    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n    \nprint(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\nprint(f'Test accuracy: {compute_accuracy(model, test_loader, device):.2f}%')","metadata":{"execution":{"iopub.status.busy":"2023-04-17T22:01:05.111603Z","iopub.execute_input":"2023-04-17T22:01:05.112706Z","iopub.status.idle":"2023-04-17T22:01:33.155732Z","shell.execute_reply.started":"2023-04-17T22:01:05.112651Z","shell.execute_reply":"2023-04-17T22:01:33.153804Z"},"trusted":true},"execution_count":119,"outputs":[{"name":"stdout","text":"Epoch: 001/015 | Batch 000/110 | Loss: 0.6449\nEpoch: 001/015 | Batch 050/110 | Loss: 0.5381\nEpoch: 001/015 | Batch 100/110 | Loss: 0.5674\ntraining accuracy: 75.73%\nvalid accuracy: 75.70%\nTime elapsed: 0.03 min\nEpoch: 002/015 | Batch 000/110 | Loss: 0.5638\nEpoch: 002/015 | Batch 050/110 | Loss: 0.5237\nEpoch: 002/015 | Batch 100/110 | Loss: 0.5357\ntraining accuracy: 75.73%\nvalid accuracy: 75.70%\nTime elapsed: 0.06 min\nEpoch: 003/015 | Batch 000/110 | Loss: 0.5640\nEpoch: 003/015 | Batch 050/110 | Loss: 0.5170\nEpoch: 003/015 | Batch 100/110 | Loss: 0.4698\ntraining accuracy: 75.73%\nvalid accuracy: 75.70%\nTime elapsed: 0.09 min\nEpoch: 004/015 | Batch 000/110 | Loss: 0.5251\nEpoch: 004/015 | Batch 050/110 | Loss: 0.6481\nEpoch: 004/015 | Batch 100/110 | Loss: 0.5014\ntraining accuracy: 75.73%\nvalid accuracy: 75.70%\nTime elapsed: 0.12 min\nEpoch: 005/015 | Batch 000/110 | Loss: 0.4784\nEpoch: 005/015 | Batch 050/110 | Loss: 0.5923\nEpoch: 005/015 | Batch 100/110 | Loss: 0.5910\ntraining accuracy: 75.74%\nvalid accuracy: 75.75%\nTime elapsed: 0.15 min\nEpoch: 006/015 | Batch 000/110 | Loss: 0.5017\nEpoch: 006/015 | Batch 050/110 | Loss: 0.5015\nEpoch: 006/015 | Batch 100/110 | Loss: 0.5724\ntraining accuracy: 75.77%\nvalid accuracy: 75.80%\nTime elapsed: 0.19 min\nEpoch: 007/015 | Batch 000/110 | Loss: 0.4620\nEpoch: 007/015 | Batch 050/110 | Loss: 0.4352\nEpoch: 007/015 | Batch 100/110 | Loss: 0.4783\ntraining accuracy: 80.16%\nvalid accuracy: 75.80%\nTime elapsed: 0.22 min\nEpoch: 008/015 | Batch 000/110 | Loss: 0.3980\nEpoch: 008/015 | Batch 050/110 | Loss: 0.4632\nEpoch: 008/015 | Batch 100/110 | Loss: 0.4675\ntraining accuracy: 82.84%\nvalid accuracy: 77.05%\nTime elapsed: 0.25 min\nEpoch: 009/015 | Batch 000/110 | Loss: 0.4237\nEpoch: 009/015 | Batch 050/110 | Loss: 0.3427\nEpoch: 009/015 | Batch 100/110 | Loss: 0.4263\ntraining accuracy: 85.24%\nvalid accuracy: 77.70%\nTime elapsed: 0.28 min\nEpoch: 010/015 | Batch 000/110 | Loss: 0.3092\nEpoch: 010/015 | Batch 050/110 | Loss: 0.3609\nEpoch: 010/015 | Batch 100/110 | Loss: 0.4119\ntraining accuracy: 87.64%\nvalid accuracy: 75.95%\nTime elapsed: 0.31 min\nEpoch: 011/015 | Batch 000/110 | Loss: 0.2626\nEpoch: 011/015 | Batch 050/110 | Loss: 0.3252\nEpoch: 011/015 | Batch 100/110 | Loss: 0.2257\ntraining accuracy: 88.42%\nvalid accuracy: 77.75%\nTime elapsed: 0.34 min\nEpoch: 012/015 | Batch 000/110 | Loss: 0.1939\nEpoch: 012/015 | Batch 050/110 | Loss: 0.3157\nEpoch: 012/015 | Batch 100/110 | Loss: 0.3545\ntraining accuracy: 91.37%\nvalid accuracy: 75.35%\nTime elapsed: 0.37 min\nEpoch: 013/015 | Batch 000/110 | Loss: 0.2551\nEpoch: 013/015 | Batch 050/110 | Loss: 0.1793\nEpoch: 013/015 | Batch 100/110 | Loss: 0.2344\ntraining accuracy: 92.66%\nvalid accuracy: 78.30%\nTime elapsed: 0.40 min\nEpoch: 014/015 | Batch 000/110 | Loss: 0.2404\nEpoch: 014/015 | Batch 050/110 | Loss: 0.1440\nEpoch: 014/015 | Batch 100/110 | Loss: 0.2433\ntraining accuracy: 94.94%\nvalid accuracy: 77.00%\nTime elapsed: 0.43 min\nEpoch: 015/015 | Batch 000/110 | Loss: 0.1665\nEpoch: 015/015 | Batch 050/110 | Loss: 0.1200\nEpoch: 015/015 | Batch 100/110 | Loss: 0.1974\ntraining accuracy: 95.79%\nvalid accuracy: 75.65%\nTime elapsed: 0.47 min\nTotal Training Time: 0.47 min\nTest accuracy: 76.62%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Loading the best Model**","metadata":{}},{"cell_type":"code","source":"path = \"/kaggle/working/lstm12.pt\"\nmodel.load_state_dict(torch.load(path))","metadata":{"execution":{"iopub.status.busy":"2023-04-17T22:02:00.266662Z","iopub.execute_input":"2023-04-17T22:02:00.267389Z","iopub.status.idle":"2023-04-17T22:02:00.284391Z","shell.execute_reply.started":"2023-04-17T22:02:00.267351Z","shell.execute_reply":"2023-04-17T22:02:00.283219Z"},"trusted":true},"execution_count":120,"outputs":[{"execution_count":120,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"markdown","source":"**F1 score at the test data**","metadata":{}},{"cell_type":"code","source":"with torch.set_grad_enabled(False):\n    print('test accuracy: ',compute_accuracy(model, test_loader, device,True))","metadata":{"execution":{"iopub.status.busy":"2023-04-17T22:02:08.872794Z","iopub.execute_input":"2023-04-17T22:02:08.873749Z","iopub.status.idle":"2023-04-17T22:02:08.998972Z","shell.execute_reply.started":"2023-04-17T22:02:08.873712Z","shell.execute_reply":"2023-04-17T22:02:08.997945Z"},"trusted":true},"execution_count":121,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.85      0.85      0.85      3030\n           1       0.54      0.54      0.54       970\n\n    accuracy                           0.78      4000\n   macro avg       0.70      0.70      0.70      4000\nweighted avg       0.78      0.78      0.78      4000\n\ntest accuracy:  tensor(77.7750, device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Macro F1 score is 0.70","metadata":{}},{"cell_type":"code","source":"with torch.set_grad_enabled(False):\n    print('test accuracy: ',compute_accuracy(model, test_loader, device,True))","metadata":{"execution":{"iopub.status.busy":"2023-04-17T22:02:24.302751Z","iopub.execute_input":"2023-04-17T22:02:24.303462Z","iopub.status.idle":"2023-04-17T22:02:24.427116Z","shell.execute_reply.started":"2023-04-17T22:02:24.303424Z","shell.execute_reply":"2023-04-17T22:02:24.425986Z"},"trusted":true},"execution_count":122,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.85      0.85      0.85      3030\n           1       0.54      0.54      0.54       970\n\n    accuracy                           0.78      4000\n   macro avg       0.70      0.70      0.70      4000\nweighted avg       0.78      0.78      0.78      4000\n\ntest accuracy:  tensor(77.7750, device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Reference\n\n1. https://www.youtube.com/watch?v=CrS-LFXEiyk\n2. https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n3. https://github.com/rasbt/stat453-deep-learning-ss21/blob/main/L15/1_lstm.ipynb","metadata":{}}]}